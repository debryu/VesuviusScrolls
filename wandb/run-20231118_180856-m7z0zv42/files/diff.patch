diff --git a/RepMode.py b/RepMode.py
index dfacc13..0d3cd7e 100644
--- a/RepMode.py
+++ b/RepMode.py
@@ -5,6 +5,7 @@ from torch.nn import functional as F
 import torch.utils.checkpoint as cp
 import math
 import matplotlib.pyplot as plt 
+from positional_encodings.torch_encodings import PositionalEncodingPermute1D
 
 class FReLU(nn.Module):
     r""" FReLU formulation. The funnel condition has a window size of kxk. (k=3 by default)
@@ -63,8 +64,13 @@ class Net(torch.nn.Module):
         self.pooling1 = torch.nn.AvgPool2d(kernel_size=2, stride=2)
         self.pooling2 = torch.nn.AvgPool2d(kernel_size=12, stride=12)
         self.pooling3 = torch.nn.AvgPool2d(kernel_size=24, stride=24)
-
+        self.decoding_layer = torch.nn.Conv3d(1,1, kernel_size = (15,21,21), stride = (1,1,1), padding = (0,10,10)).to(self.device)
+        self.decoding_layer_macro = torch.nn.Conv3d(1,1, kernel_size = (19,63,63), stride = (1,1,1), padding = (0,31,31)).to(self.device)
+        self.normalization = torch.nn.BatchNorm2d(1)
         self.final_activation = torch.nn.Sigmoid()
+        self.logit = nn.Conv2d(64, 1, 1, 1, 0)
+        self.dropout1 = nn.Dropout(0.25)
+        self.dropout2 = nn.Dropout(0.1)
         #self.condensing2 = torch.nn.Conv2d(64*4,1, kernel_size=25, stride=1)
 
     def one_hot_task_embedding(self, task_id):
@@ -79,25 +85,51 @@ class Net(torch.nn.Module):
         task_emb = self.one_hot_task_embedding(t)
 
         # encoding
+        x = self.dropout1(x)
         x, x_skip1 = self.encoder_block1(x, task_emb)
         x, x_skip2 = self.encoder_block2(x, task_emb)
+        #x = self.dropout(x)
         x, x_skip3 = self.encoder_block3(x, task_emb)
         x, x_skip4 = self.encoder_block4(x, task_emb)
-
+        #x = self.dropout(x)
         # bottle
         x = self.bottle_block(x, task_emb)
 
         # decoding
+        #x = self.dropout(x)
         x = self.decoder_block4(x, x_skip4, task_emb)
         x = self.decoder_block3(x, x_skip3, task_emb)
+        #x = self.dropout(x)
         x = self.decoder_block2(x, x_skip2, task_emb)
         x = self.decoder_block1(x, x_skip1, task_emb)
         outputs = self.conv_out(x, task_emb)
+        outputs = self.dropout2(outputs)
         #print(outputs.shape)
         # Squeeze to 2D
         outputs = outputs.squeeze(1)
         #outputs = torch.mean(outputs, dim=1).unsqueeze(1)
-        
+         # [N, 64, 64, 64] 
+        outputs = self.logit(outputs)
+        # First reshape the output to [N, 64, 64, 64] -> [N, 64, 64*64]
+        #in_pos_encoding = outputs.reshape(outputs.shape[0], 64, 64*64)
+        # Use positional encoding instead
+        #positional_encoding = PositionalEncodingPermute1D(in_pos_encoding.shape[1])
+        # Dim 1 cause dim0 is the batch, we want to collapse Z which is dim1
+        #weights = positional_encoding(in_pos_encoding)
+        '''
+        #decoding_layer = torch.nn.Conv3d(1,1, kernel_size = (15,21,21), stride = (1,1,1), padding = (0,10,10)).to(self.device)
+        outputs = self.decoding_layer(outputs.unsqueeze(1))
+        # Remove the first dimension
+        outputs = outputs
+        # Sum all the Z layers
+        outputs = torch.sum(outputs, dim=2)
+        # Normalize the output
+        outputs = self.normalization(outputs)
+        # Shape [N, 1, 64, 64]
+        outputs = self.final_activation(outputs)
+        '''
+
+        '''
         outputs_scale_1 = self.condensing1(outputs)
         # [N, 64, 64, 64] -> [N, 1, 64, 64]
         outputs_scale_1 = self.pooling1(outputs_scale_1)
@@ -113,9 +145,11 @@ class Net(torch.nn.Module):
         outputs = torch.mean(torch.stack([outputs_scale_1, outputs_scale_2, outputs_scale_3], dim=1), 1)
         
         outputs = self.final_activation(outputs)
+        '''
         #print(outputs.shape)
         #outputs = outputs.squeeze(-1).squeeze(-1).reshape(outputs.shape[0],1,64,64)
         #print(outputs.shape)
+        
         return outputs
 
 
diff --git a/__pycache__/RepMode.cpython-39.pyc b/__pycache__/RepMode.cpython-39.pyc
index 7b92890..94327de 100644
Binary files a/__pycache__/RepMode.cpython-39.pyc and b/__pycache__/RepMode.cpython-39.pyc differ
diff --git a/__pycache__/vesuvius_dataloader.cpython-39.pyc b/__pycache__/vesuvius_dataloader.cpython-39.pyc
index 742ada3..b2bd892 100644
Binary files a/__pycache__/vesuvius_dataloader.cpython-39.pyc and b/__pycache__/vesuvius_dataloader.cpython-39.pyc differ
diff --git a/main.py b/main.py
index c05296f..c8a28a9 100644
--- a/main.py
+++ b/main.py
@@ -14,9 +14,12 @@ import utils.losses as losses
 import wandb
 from piq import SSIMLoss
 import math
+import segmentation_models_pytorch as smp
+from warmup_scheduler import GradualWarmupScheduler
 
-LOAD_MODEL = True
-STOPPING_EPOCH = 8000000 # Must be multiple of 8 to match with the gradient accumulation steps
+LOAD_MODEL = False
+s_epoch = 26
+STOPPING_EPOCH = 500 
 RUN_EVAL = False
 EVAL_WINDOW = 150
 WANDB = True
@@ -32,9 +35,9 @@ opts = Options(
     gpu_ids=[0],
     path_load_dataset='data/all_data',
     num_epochs=100000,
-    batch_size = 6,
+    batch_size = 7,
     eval_batch_size = 2,
-    lr=0.00001,
+    lr=0.00002,
     criterion=nn.MSELoss(reduction='none'),
     #criterion = losses.dice_loss(),
     interval_val=1,
@@ -53,6 +56,50 @@ opts = Options(
     device='cuda',
 )
 
+'''
+USE WARMUP AND COSINE ANNEALING
+WARMUP: pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git
+'''
+
+class GradualWarmupSchedulerV2(GradualWarmupScheduler):
+    """
+    https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965
+    """
+    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):
+        super(GradualWarmupSchedulerV2, self).__init__(
+            optimizer, multiplier, total_epoch, after_scheduler)
+
+    def get_lr(self):
+        if self.last_epoch > self.total_epoch:
+            if self.after_scheduler:
+                if not self.finished:
+                    self.after_scheduler.base_lrs = [
+                        base_lr * self.multiplier for base_lr in self.base_lrs]
+                    self.finished = True
+                return self.after_scheduler.get_lr()
+            return [base_lr * self.multiplier for base_lr in self.base_lrs]
+        if self.multiplier == 1.0:
+            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]
+        else:
+            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]
+
+
+def get_scheduler(optimizer):
+    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(
+        optimizer, 10, eta_min=1e-7)
+    scheduler = GradualWarmupSchedulerV2(
+        optimizer, multiplier=1.0, total_epoch=3, after_scheduler=scheduler_cosine)
+
+    return scheduler
+
+def scheduler_step(scheduler, epoch):
+    scheduler.step(epoch)
+
+loss_func1 = smp.losses.DiceLoss(mode='binary')
+loss_func2= smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.25)
+loss_func= lambda x,y:0.5 * loss_func1(x,y)+0.5*loss_func2(x,y)
+max_grad_norm = 2.0
+
 def train(model, dataloader, opts, epoch, optimizer, scaler, use_wandb,  grad_acc_steps=8, early_stopping=STOPPING_EPOCH):
     model.train()
     train_loss = []
@@ -82,11 +129,12 @@ def train(model, dataloader, opts, epoch, optimizer, scaler, use_wandb,  grad_ac
             #wei = torch.tensor(wei).to(self.device)
             #tensor = torch.tensor(np.ones((4, 4))).to(self.device)
             #print(tensor*wei)
-            loss = loss * weight
-            focus_loss = torch.mean(loss)*22222
-            image_loss = SSIMLoss(data_range=1.0)
-            img_rec_loss = image_loss(outputs, targets)/2
-            class_loss = losses.dice_loss_weight_noMask(outputs, targets)*4
+            #loss = loss * weight
+            #focus_loss = torch.mean(loss)*22222
+            #image_loss = SSIMLoss(data_range=1.0)
+            #img_rec_loss = image_loss(outputs, targets)/2
+            #class_loss = losses.dice_loss_weight_noMask(outputs, targets)*4
+            loss = loss_func(outputs, targets)
             if i == 0 and epoch % 5 == 0 and False:
                 images = [outputs[0].squeeze(0).cpu().detach().numpy(), target[0].cpu().numpy(), outputs[1].squeeze(0).cpu().detach().numpy(), target[1].cpu().numpy()]
                 fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))
@@ -101,17 +149,17 @@ def train(model, dataloader, opts, epoch, optimizer, scaler, use_wandb,  grad_ac
                 # Show the plot
                 plt.show()
 
-            if use_wandb and False:
+            if use_wandb:
                 wandb.log({
-                            '[TRAIN] focus_loss/iter': focus_loss.item(),
-                            '[TRAIN] rec_loss/iter': img_rec_loss.item(),
-                            '[TRAIN] class_loss/iter': class_loss.item(),
+                            '[TRAIN] loss/iter': loss.item(),
                            })
 
-        loss = (img_rec_loss + class_loss + focus_loss)/3
+        #loss = (img_rec_loss + class_loss + focus_loss)/3
+
         train_loss.append(loss.item())
         #print(loss)
         scaler.scale(loss).backward()
+        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
         scaler.step(optimizer)
         scaler.update()
         optimizer.zero_grad()
@@ -142,17 +190,18 @@ def evaluate(model, dataloader, opts, epoch, prev_image_folder = "G:/VS_CODE/CV/
             # Add to the list of predictions
             subsection_predictions.append(outputs[:,:,start:end,start:end])
             target = target.unsqueeze(1).to(torch.float64)
-            loss = opts.criterion(outputs, target).squeeze()
+            #loss = opts.criterion(outputs, target).squeeze()
+            loss = loss_func(outputs, target)
             # Apply a gaussian filter to the output
-            center = ((64-1)/2,(64-1)/2)
-            weight = losses.gaussian_kernel(64,center, sigma = losses.SIGMA)
-            weight = torch.tensor(weight).to(opts.device).to(torch.float64)
-            loss = loss * weight
-            focus_loss = torch.mean(loss)*22222
-            class_loss = losses.dice_loss_weight_noMask(outputs, target)*4
-            image_loss = SSIMLoss(data_range=1.0)
-            img_rec_loss = image_loss(outputs, target)/2
-            loss = (img_rec_loss + class_loss + focus_loss)/3
+            #center = ((64-1)/2,(64-1)/2)
+            #weight = losses.gaussian_kernel(64,center, sigma = losses.SIGMA)
+            #weight = torch.tensor(weight).to(opts.device).to(torch.float64)
+            #loss = loss * weight
+            #focus_loss = torch.mean(loss)*22222
+            #class_loss = losses.dice_loss_weight_noMask(outputs, target)*4
+            #image_loss = SSIMLoss(data_range=1.0)
+            #img_rec_loss = image_loss(outputs, target)/2
+            #loss = (img_rec_loss + class_loss + focus_loss)/3
             eval_loss.append(loss.item())
     # Concatenate all the pixels
     subsection_predictions = torch.cat(subsection_predictions, dim=0).squeeze(1)
@@ -195,7 +244,6 @@ def main():
     model = Net(opts).to(opts.device)
     
     if LOAD_MODEL:
-        s_epoch = 7
         model.load_state_dict(torch.load(models_folder + f"model_DeBData_{s_epoch}.p"))
         training_range = range(s_epoch+1,opts.num_epochs)
     else:
@@ -203,6 +251,7 @@ def main():
 
     #DEFINE OPTIMIZER
     optimizer = torch.optim.AdamW(model.parameters(), lr=opts.lr)
+    scheduler = get_scheduler(optimizer)
     #optimizer = torch.optim.Adam(model.parameters(), lr=opts.lr)
     #optimizer = torch.optim.SGD(model.parameters(), lr=opts.lr)
     scaler = GradScaler()
@@ -213,12 +262,13 @@ def main():
     #TRAIN & EVAL LOOP
     for epoch in training_range:  
         losses = train(model, train_dl, opts, epoch, optimizer, scaler, use_wandb=WANDB)
+        scheduler_step(scheduler, epoch)
         final_loss = np.mean(losses)
-        stats = {
-                    '[TRAIN] loss/epoch': final_loss,
-                 }
         if WANDB:
-            wandb.log(stats)
+            print("logging")
+            wandb.log({
+                        '[TRAIN] loss/epoch': final_loss,
+                      })
         if final_loss < best_model_loss:
             best_model_loss = final_loss
             # Save the model
@@ -232,11 +282,10 @@ def main():
         gc.collect()
         if (epoch+1) % opts.interval_val == 0:
             losses = evaluate(model, validation_dl, opts, epoch)
-            stats = {
-                        '[VAL] loss/epoch': np.mean(losses),
-                    }
             if WANDB:
-                wandb.log(stats)
+                wandb.log({
+                            '[VAL] loss/epoch': np.mean(losses),
+                        })
             gc.collect()
 
 
diff --git a/scripts/dataset/sanity_check.py b/scripts/dataset/sanity_check.py
index 564e5ad..330d709 100644
--- a/scripts/dataset/sanity_check.py
+++ b/scripts/dataset/sanity_check.py
@@ -6,12 +6,15 @@ import numpy as np
 import torch
 from tqdm import tqdm
 import torch.utils.data as data
+import math
 
 train_ds = dataloader.train_ds
 n_samples = len(train_ds)
+batch_size = 7
+iters = math.floor(n_samples/batch_size)
 print(n_samples)
 #LOAD DATA
-train_dl = data.DataLoader(train_ds, batch_size = 2, shuffle=True)
+train_dl = data.DataLoader(train_ds, batch_size = batch_size, shuffle=False)
 
 
 total_pixels = 0
@@ -19,21 +22,20 @@ white_pixels = 0
 frequencies = []
 mean_pic = torch.zeros((64,64))
 for i,batch in enumerate(tqdm(train_dl)):
-    chunks, labels, tasks, id = batch
-    lab1,lab2 = labels
-    id1,id2 = id
-    id1 = int(id1)
-    id2 = int(id2)
+    _, labels, _, id = batch
+    
     #print(id1,id2)
-    total_pixels += 64*64*2
-    white_pixels += torch.sum(lab1) + torch.sum(lab2)
-    mean_pic += lab1 + lab2
+    ttp = 64*64*labels.shape[0]
+    total_pixels += ttp
+    twp = torch.sum(labels)
+    white_pixels += twp.item()
+    mean_pic = torch.add(mean_pic,torch.sum(labels,dim=0))
     
     #print(dataloader.dataset['object'][id1])
     #print(dataloader.dataset['object'][id2])
     #print(torch.max(lab1),torch.max(lab2))
-    frequencies.append(((torch.sum(lab1) + torch.sum(lab2))/(64*64*2)).item())
-    if(i > 5000):
+    frequencies.append(twp/ttp)
+    if(i == len(train_dl)-2):
         break
 
 # Plot an histogram of the frequencies
diff --git a/utils/__pycache__/losses.cpython-39.pyc b/utils/__pycache__/losses.cpython-39.pyc
index 0afd6cb..b51086a 100644
Binary files a/utils/__pycache__/losses.cpython-39.pyc and b/utils/__pycache__/losses.cpython-39.pyc differ
diff --git a/utils/losses.py b/utils/losses.py
index 633eafb..410d363 100644
--- a/utils/losses.py
+++ b/utils/losses.py
@@ -5,7 +5,7 @@ import matplotlib.pyplot as plt
 
 
 WINDOW_SIZE = 64
-SIGMA = 15
+SIGMA = 64
 
 
 
@@ -38,6 +38,15 @@ def dice_loss_weight(score,target,mask = weight):
     loss = 1 - loss
     return loss
 
+def dice_loss_weight_noMask(score,target):
+      target = target.float()
+      smooth = 1e-5
+      intersect = torch.sum(score * target)
+      y_sum = torch.sum(target * target)
+      z_sum = torch.sum(score * score)
+      loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)
+      loss = 1 - loss
+      return loss
 
 def wce(logits,target,weights,batch_size=2,H=64,W=64,D=64):
   # Calculate log probabilities
@@ -77,4 +86,22 @@ class dice_loss:
       z_sum = torch.sum(score * score * mask)
       loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)
       loss = 1 - loss
+      return loss
+  
+  def dice_loss_weight_noMask(self, score,target):
+      if mask is None:
+        mask = self.gaussian_kernel
+
+      # Print the mask for sanity check
+      #plt.imshow(mask.numpy())
+      #plt.show()
+
+      target = target.float()
+      smooth = 1e-5
+      mask = mask.to(self.device)
+      intersect = torch.sum(score * target)
+      y_sum = torch.sum(target * target)
+      z_sum = torch.sum(score * score)
+      loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)
+      loss = 1 - loss
       return loss
\ No newline at end of file
diff --git a/vesuvius_dataloader.py b/vesuvius_dataloader.py
index 0c9db7b..576799d 100644
--- a/vesuvius_dataloader.py
+++ b/vesuvius_dataloader.py
@@ -5,6 +5,7 @@ import numpy as np
 import glob
 import PIL.Image as Image
 import torch.utils.data as data
+from torch.utils.data import Dataset, DataLoader
 import matplotlib.pyplot as plt
 import matplotlib.patches as patches
 from tqdm import tqdm
@@ -12,6 +13,7 @@ import os
 import time
 import gc
 import cv2 as cv
+from utils.dataloader_fn import create_edge_mask, extract_training_points, normalize_training_points, extract_random_points, extract_training_and_val_points
 
 FROM = 0
 WINDOW = 32
@@ -42,166 +44,14 @@ for i,n in enumerate(names):
     fragments.append(temp)
 ###############################################################################
 
-class SubvolumeDataset(data.Dataset):
-    def __init__(self, surfaces, labels, class_labels, coordinates,task = 0):
-        self.surfaces = surfaces
-        self.labels = labels
-        self.class_labels = class_labels
-        self.coordinates = coordinates
-        self.task = task
-    def __len__(self):
-        #print(self.coordinates)
-        return len(self.coordinates)
-    def __getitem__(self, index):
-        scroll_id = self.class_labels[index]
-        #print(index)
-        #print(scroll_id)
-        label = self.labels[scroll_id]
-        #print('label shape: ', label.shape)
-        #print(len(self.surfaces))
-        image_stack = self.surfaces[scroll_id]
-        #print(image_stack.shape)
-        y, x = self.coordinates[index]
-        #print(y, x)
-        #print(self.image_stack[:, y-WINDOW:y+WINDOW+ODD_WINDOW, x-WINDOW:x+WINDOW+ODD_WINDOW].shape)
-        
-        subvolume = image_stack[:, y-WINDOW:y+WINDOW+ODD_WINDOW, x-WINDOW:x+WINDOW+ODD_WINDOW]
-        subvolume = torch.tensor(subvolume)
-        subvolume = subvolume.view(1, SCAN_DEPTH, WINDOW*2+ODD_WINDOW, WINDOW*2+ODD_WINDOW)
-        
-        inklabel = label[y, x].view(1)
-        inkpatch = label[y-WINDOW:y+WINDOW+ODD_WINDOW, x-WINDOW:x+WINDOW+ODD_WINDOW]
-       
-        #print(inkpatch)
-        #print(current_task)
-        #plt.imshow(inkpatch.cpu().numpy())
-        #plt.show()
-        
-        # Flipping
-        '''
-        if rand > 0.5:
-            if rand > 0.75:
-                subvolume = torch.flip(subvolume, [3])
-                inkpatch = torch.flip(inkpatch, [1])
-            else:
-                subvolume = torch.flip(subvolume, [2])
-                inkpatch = torch.flip(inkpatch, [0])
-            #print(inkpatch.shape)
-        '''
-        #print(inkpatch.shape)
-        return subvolume, inkpatch, self.task, scroll_id
-        #return subvolume, inklabel, self.task
-
-
-'''
-
-'''
-def extract_random_points(FRAG_MASK, num_points):
-    not_border = np.zeros(FRAG_MASK.shape, dtype=bool)
-    not_border[WINDOW:FRAG_MASK.shape[0]-WINDOW, WINDOW:FRAG_MASK.shape[1]-WINDOW] = True
-    arr_mask = np.array(FRAG_MASK) * not_border
-    arr_mask = np.argwhere(arr_mask)
-    random_points = np.random.choice(arr_mask.shape[0], num_points, replace=False)
-    return arr_mask[random_points]
-
-
-def extract_test_points(FRAG_MASK):
-    not_border = np.zeros(FRAG_MASK.shape, dtype=bool)
-    not_border[WINDOW:FRAG_MASK.shape[0]-WINDOW, WINDOW:FRAG_MASK.shape[1]-WINDOW] = True
-    arr_mask = np.array(FRAG_MASK) * not_border
-    test = np.ones(FRAG_MASK.shape, dtype=bool) * arr_mask
-    test = np.argwhere(test)
-    return test
-
-def extract_training_and_val_points(FRAG_MASK, validation_rect= (1175,3602,63,63)):
-    not_border = np.zeros(FRAG_MASK.shape, dtype=bool)
-    not_border[WINDOW:FRAG_MASK.shape[0]-WINDOW, WINDOW:FRAG_MASK.shape[1]-WINDOW] = True
-    arr_mask = np.array(FRAG_MASK) * not_border
-    
-    # Initialize the validation patch as big as the whole mask
-    validation = np.zeros(FRAG_MASK.shape, dtype=bool) * arr_mask
-    # and then set the inner rectangle to True
-    validation[validation_rect[1]:validation_rect[1]+validation_rect[3]+1, validation_rect[0]:validation_rect[0]+validation_rect[2]+1] = True
-    # Sets all the pixels inside the mask to True
-    train = np.ones(FRAG_MASK.shape, dtype=bool) * arr_mask
-    # and then set the pixels within the validation rectangle to False
-    train[validation_rect[1]:validation_rect[1]+validation_rect[3]+1, validation_rect[0]:validation_rect[0]+validation_rect[2]+1] = False
-    train = np.argwhere(train)
-    validation = np.argwhere(validation)
-    # Create a subset for faster rendering
-    val = extract_render_points(validation, validation_rect[2]+1, validation_rect[3]+1)
-    return train, val
-
-def extract_training_points(FRAG_MASK):
-    not_border = np.zeros(FRAG_MASK.shape, dtype=bool)
-    not_border[WINDOW:FRAG_MASK.shape[0]-WINDOW, WINDOW:FRAG_MASK.shape[1]-WINDOW] = True
-    arr_mask = np.array(FRAG_MASK) * not_border
-    train = np.ones(FRAG_MASK.shape, dtype=bool) * arr_mask
-    train = np.argwhere(train)
-    return train
-
-
-def extract_render_points(pixels, original_width, original_height, stride_H = 64, stride_W = 64):
-    W = original_width
-    H = original_height
-    pixels_to_render = []
-    
-    points_per_row = int(W/stride_W)
-    points_per_col = int(H/stride_H)
-    
-    offset = (stride_H/2 - 1)*W + stride_W/2
-    for i in range(0, points_per_col):
-        for j in range(0, points_per_row):
-            #print(f'i:{i}/122 - j:{j}/82  -- Row: {pix_count_row}, Col: {pix_count_col}')
-            index = j*stride_W + stride_W*(i)*W
-            pixels_to_render.append(pixels[index + int(offset)])
-            #print(len(pixels_to_render))
-    
-    return pixels_to_render
-
-def old_extract_render_points(pixels, original_width, original_height, FOV = 64):
-    W = original_width
-    H = original_height
-    pixels_to_render = []
-    points_per_row = int(W/FOV)
-    points_per_col = int(H/FOV)
-    pix_count_row = 32
-    pix_count_col = 31
-    #print("start")
-    val_pixels_0overlap = []
-    for i in range(0, points_per_col):
-        for j in range(0, points_per_row):
-            #print(f'i:{i}/122 - j:{j}/82  -- Row: {pix_count_row}, Col: {pix_count_col}')
-            pixels_to_render.append(pixels[pix_count_row + W*pix_count_col])
-            #print(len(pixels_to_render))
-            pix_count_row += 64
-        pix_count_col += 64
-        pix_count_row = 32
-    return pixels_to_render
-
-def create_edge_mask(image_path):
-    img = cv.imread(image_path, cv.IMREAD_GRAYSCALE)
-    edges = cv.Canny(img, 100, 200)
-    # Define a kernel for dilation
-    kernel = np.ones((1, 1), np.uint8)  # You can adjust the size of the kernel
-    # Dilate the edges to increase the thickness
-    dilated_edges = cv.dilate(edges, kernel, iterations=1)
-    edge_mask = np.array(torch.from_numpy(np.array(dilated_edges)).gt(155).float().to('cpu'))
-    edge_mask_pt = torch.from_numpy(np.array(dilated_edges)).gt(155).float().to(device)
-    #plt.imshow(edge_mask)
-    #plt.show()
-    return edge_mask, edge_mask_pt
-
-
-
 '''
 DEFINE MANUALLY ALL THE LABEL FILES
 
 '''
 
 FRAG1_MASK = np.array(Image.open( base_folder + f"Fragments/Frag1/mask.png").convert('1'))
-FRAG2_MASK1 = np.array(Image.open( base_folder + f"Fragments/Frag2/part1_mask.png").convert('1'))
-FRAG2_MASK2 = np.array(Image.open( base_folder + f"Fragments/Frag2/part2_mask.png").convert('1'))
+FRAG2_MASKa = np.array(Image.open( base_folder + f"Fragments/Frag2/part1_mask.png").convert('1'))
+FRAG2_MASKb = np.array(Image.open( base_folder + f"Fragments/Frag2/part2_mask.png").convert('1'))
 FRAG3_MASK = np.array(Image.open( base_folder + f"Fragments/Frag3/mask.png").convert('1'))
 FRAG4_MASK = np.array(Image.open( base_folder + f"Fragments/Frag4/mask.png").convert('1'))
 FRAG1_LABEL_PNG = Image.open(base_folder + f"Fragments/Frag1/inklabels.png")
@@ -230,6 +80,8 @@ FRAG4_EDGES_LABEL, FRAG4_EDGES_LABEL_PT = create_edge_mask(base_folder + f"Fragm
 label_as_img = Image.open(base_folder + f"Fragments/Frag1/inklabels.png")
 
 # Crop the rectangle from the original image
+#(1175,3602,63,63)
+print("Evaluation window: ", EVAL_WINDOW)
 val_label = label_as_img.crop((1175, 3602, 1175+EVAL_WINDOW, 3602+EVAL_WINDOW))
 
 # Save the cropped image
@@ -250,10 +102,20 @@ training_points_2a = extract_training_points(FRAG2_EDGES_LABELa)
 training_points_2b = extract_training_points(FRAG2_EDGES_LABELb)
 training_points_3 = extract_training_points(FRAG3_EDGES_LABEL)
 training_points_4 = extract_training_points(FRAG4_EDGES_LABEL)
-random_points_2a = extract_random_points(FRAG2_EDGES_LABELa, 100)
-random_points_3 = extract_random_points(FRAG3_EDGES_LABEL, 363)
-random_points_4 = extract_random_points(FRAG4_EDGES_LABEL, 100)
-#563
+
+# Normalize the training points
+training_points_1 = normalize_training_points(training_points_1)
+training_points_2a = normalize_training_points(training_points_2a)
+training_points_2b = normalize_training_points(training_points_2b)
+training_points_3 = normalize_training_points(training_points_3)
+training_points_4 = normalize_training_points(training_points_4)
+
+random_points_1 = extract_random_points(FRAG1_LABEL, 60)
+random_points_2a = extract_random_points(FRAG2_LABELa, 50)
+random_points_2b = extract_random_points(FRAG2_LABELb, 50)
+random_points_3 = extract_random_points(FRAG3_LABEL, 60)
+random_points_4 = extract_random_points(FRAG4_LABEL, 60)
+
 
 hpp_F1_BLACK = [
     (2383,1818),
@@ -295,35 +157,71 @@ hpp_F2a_BLACK = [
     (3480,3384),
 ]
 
+black_squaresf102 = np.load(base_folder + 'Fragments_dataset/special_dataset/black02/black_coords_f1.npy')
+black_squaresf2a01 = np.load(base_folder + 'Fragments_dataset/special_dataset/black01/black_coords_f2a.npy')
+black_squaresf301_more = np.load(base_folder + 'Fragments_dataset/special_dataset/black01/black_coords_f3.npy') 
+
+
+
+step = 64
 # Concatenate coordinates
-all_coordinates = np.concatenate([training_points_1[0::64], 
-                                  training_points_2a[0::64], 
-                                  training_points_2b[0::64], 
-                                  training_points_3[0::64], 
-                                  training_points_4[0::64], 
+all_coordinates = np.concatenate([training_points_1[0::step], 
+                                  training_points_2a[0::step], 
+                                  training_points_2b[0::step], 
+                                  training_points_3[0::step], 
+                                  training_points_4[0::step], 
                                   hpp_F1_BLACK, 
                                   hpp_F1_WHITE,
                                   hpp_F2a_BLACK,
-                                  random_points_2a,
-                                  random_points_3,
-                                  random_points_4,
+                                  #random_points_1,
+                                  #random_points_2a,
+                                  #random_points_2b,
+                                  #random_points_3,
+                                  #random_points_4,
+                                  black_squaresf102[0::2],
+                                  black_squaresf2a01[0::2],
+                                  black_squaresf301_more[0::2],
                                   ])
+
+print(len(training_points_1[0::step]))
+print(len(training_points_2a[0::step]))
+print(len(training_points_2b[0::step]))
+print(len(training_points_3[0::step]))
+print(len(training_points_4[0::step]))
+print(len(hpp_F1_BLACK))
+print(len(hpp_F1_WHITE))
+print(len(hpp_F2a_BLACK))
+#print(len(random_points_1))
+#print(len(random_points_2a))
+#print(len(random_points_2b))
+#print(len(random_points_3))
+#print(len(random_points_4))
+print(len(black_squaresf102))
+print(len(black_squaresf2a01))
+print(len(black_squaresf301_more))
+
+
 # Create an array for class labels
-class_labels = np.array(  [0] * len(training_points_1[0::64]) 
-                        + [1] * len(training_points_2a[0::64]) 
-                        + [2] * len(training_points_2b[0::64]) 
-                        + [3] * len(training_points_3[0::64]) 
-                        + [4] * len(training_points_4[0::64])
+class_labels = np.array(  [0] * len(training_points_1[0::step]) 
+                        + [1] * len(training_points_2a[0::step]) 
+                        + [2] * len(training_points_2b[0::step]) 
+                        + [3] * len(training_points_3[0::step]) 
+                        + [4] * len(training_points_4[0::step])
                         + [0] * len(hpp_F1_BLACK) 
                         + [0] * len(hpp_F1_WHITE)    
-                        + [1] * len(hpp_F2a_BLACK)  
-                        + [1] * len(random_points_2a)
-                        + [3] * len(random_points_3)   
-                        + [4] * len(random_points_4)                                                                                                                                                                                                                                                
+                        + [1] * len(hpp_F2a_BLACK) 
+                        #+ [0] * len(random_points_1)     
+                        #+ [1] * len(random_points_2a)
+                        #+ [2] * len(random_points_2b)
+                        #+ [3] * len(random_points_3)   
+                        #+ [4] * len(random_points_4)   
+                        + [0] * len(black_squaresf102[0::2])
+                        + [1] * len(black_squaresf2a01[0::2])
+                        + [3] * len(black_squaresf301_more[0::2])                                                                                                                                                                                                                                             
                        )
 # Store all the labels
 all_labels = [FRAG1_LABEL, FRAG2_LABELa, FRAG2_LABELb, FRAG3_LABEL, FRAG4_LABEL]
-
+all_masks = [FRAG1_MASK, FRAG2_MASKa, FRAG2_MASKb, FRAG3_MASK, FRAG4_MASK]
 
 '''
 # Only frag1
@@ -332,6 +230,65 @@ class_labels = np.array([0] * len(training_points_3[0::64]))
 all_labels = [FRAG3_LABEL]
 '''
 
+
+class SubvolumeDataset(data.Dataset):
+    def __init__(self, surfaces, labels, class_labels, coordinates,task = 0):
+        self.surfaces = surfaces
+        self.labels = labels
+        self.class_labels = class_labels
+        self.coordinates = coordinates
+        self.task = task
+        self.epoch = 0
+        self.masks = []
+    def __len__(self):
+        #print(self.coordinates)
+        return len(self.coordinates)
+    def __getitem__(self, index):
+        scroll_id = self.class_labels[index]
+        #print(index)
+        #print(scroll_id)
+        label = self.labels[scroll_id]
+        #print('label shape: ', label.shape)
+        #print(len(self.surfaces))
+        image_stack = self.surfaces[scroll_id]
+        #print(image_stack.shape)
+        y, x = self.coordinates[index]
+        #print(y, x)
+        #print(self.image_stack[:, y-WINDOW:y+WINDOW+ODD_WINDOW, x-WINDOW:x+WINDOW+ODD_WINDOW].shape)
+        
+        if self.epoch > 10:
+            # Get random points from the image
+            a = 10
+
+        subvolume = image_stack[:, y-WINDOW:y+WINDOW+ODD_WINDOW, x-WINDOW:x+WINDOW+ODD_WINDOW]
+        subvolume = torch.tensor(subvolume)
+        subvolume = subvolume.view(1, SCAN_DEPTH, WINDOW*2+ODD_WINDOW, WINDOW*2+ODD_WINDOW)
+        
+        inklabel = label[y, x].view(1)
+        inkpatch = label[y-WINDOW:y+WINDOW+ODD_WINDOW, x-WINDOW:x+WINDOW+ODD_WINDOW]
+       
+        #print(inkpatch)
+        #print(current_task)
+        #plt.imshow(inkpatch.cpu().numpy())
+        #plt.show()
+        
+        # Flipping
+        '''
+        if rand > 0.5:
+            if rand > 0.75:
+                subvolume = torch.flip(subvolume, [3])
+                inkpatch = torch.flip(inkpatch, [1])
+            else:
+                subvolume = torch.flip(subvolume, [2])
+                inkpatch = torch.flip(inkpatch, [0])
+            #print(inkpatch.shape)
+        '''
+        #print(inkpatch.shape)
+        return subvolume, inkpatch, self.task, scroll_id
+        #return subvolume, inklabel, self.task
+
+
+
 train_ds = SubvolumeDataset(fragments, all_labels, class_labels, all_coordinates)
 total_train_iters = len(all_coordinates)
 #print(validation_points_1)
